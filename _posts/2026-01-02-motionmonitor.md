---
layout: post
title: Wearable Motion and Position Monitor
subtitle: For Sign Language Interpretation
#gh-repo: daattali/beautiful-jekyll
#gh-badge: [star, fork, follow]
thumbnail-img: /assets/img/embed_ml/f25/motionmonitor/IMG_2570.jpg
tags: [projects_fall_25]
comments: false
mathjax: true
author: Adwoa Asare, Tianyi He, and Shao-Ju Wang
---

We wanted to create a hand position/movement classification
glove with applications in human-
computer interaction. This is useful for cases where hands need to be
tracked without the use of a camera. We use edge processing to maintain user bio-data
private and real-time responsiveness. We use multi-sensor fusion to improve resolution
and classification accuracy. The glove classifies sign language gestures and allows the user to interact with a virtual agent through sign language. Real-world applications would allow the glove to translate American Sign Language (ASL) to English or other spoken languages.  

<p align="center"> <img src="/assets/img/embed_ml/f25/motionmonitor/IMG_2568.gif" width="90%" height="90%"> </p>     
[Get the complete PDF report.](/assets/img/embed_ml/f25/motionmonitor/wearablemotionpositioningmonitor_125155_13686859_Demo.pdf)

